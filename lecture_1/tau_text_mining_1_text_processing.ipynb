{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwK5-9FIB-lu"
      },
      "source": [
        "# Text Processing - for TAU Text Mining (for MBA) 24/25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1kiO9kACE6s"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7QG7sxmoCIvN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTfaCIzdCLPA"
      },
      "source": [
        "## 1. Importing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUHJ5SZCQLza"
      },
      "source": [
        "Taken from:\n",
        "https://www.kaggle.com/datasets/maher3id/restaurant-reviewstsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbfhlXXOQLzb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCK6vQ5QCQJe"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlGd3SGKQLzb"
      },
      "outputs": [],
      "source": [
        "dataset.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwbmoQbtQLzb"
      },
      "source": [
        "### Q1: Extract random sentences\n",
        "\n",
        "Extract a random sample of 10 sentences as a list of strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9dcHIBVQLzb"
      },
      "source": [
        "## 2. Simple Bag-of-Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B_lRiPtQLzb"
      },
      "source": [
        "### Q2: Build a BoW-er\n",
        "\n",
        "Write a function that turns a string sentence into a dict-based BoW representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEO0wBgKQLzb"
      },
      "source": [
        "**Hint:** Use https://www.nltk.org/api/nltk.tokenize.word_tokenize.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyGYWLHXQLzb"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def bower(sent_str: str) -> Dict[str, int]:\n",
        "    # Fix me!\n",
        "    return {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv5ld8lWQLzc"
      },
      "source": [
        "bower(sent_sample[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTuCu4bsQLzc"
      },
      "source": [
        "### Q3: Generate a single BoW-dict representation for your sentence sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtYPmKLuQLzc"
      },
      "source": [
        "### Q4: How many unique words in your sentence sample?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMKBv6CQLzc"
      },
      "source": [
        "## 3. BoW-based vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NebDImB6QLzc"
      },
      "source": [
        "Here is how we can use existing `sklearn` features to get one-hot-encoded vector BoW representations of our sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwW-_pyiQLzc"
      },
      "source": [
        "https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAJ8GEgtQLzc"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = 3000, lowercase=False)\n",
        "X = cv.fit_transform(sent_sample).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CqZyPHUQLzc"
      },
      "outputs": [],
      "source": [
        "X[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJeLzD4UQLzc"
      },
      "source": [
        "### Q5: How many unique words in your sentence sample, according to CountVectorizer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYQ12kJWQLzc"
      },
      "source": [
        "Why is the number (probably) lower?\n",
        "\n",
        "Because of the way `CountVectorizer` separates strings into tokens; see the documentation for `token_pattern`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDEwlSP3QLzd"
      },
      "source": [
        "``token_patternstr or None, default=r”(?u)\\b\\w\\w+\\b”``\n",
        "\n",
        "  Regular expression denoting what constitutes a “token”, only used if analyzer == 'word'. The default regexp select tokens of 2 or\n",
        "  more alphanumeric characters **(punctuation is completely ignored and always treated as a token separator)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qekztq71CixT"
      },
      "source": [
        "## 4. Text Preprocessing / Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SBlH7xlQLzd"
      },
      "source": [
        "### Q6: Build a document preprocessing function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmN5vag8QLzd"
      },
      "source": [
        "Make sure to:\n",
        "1. Lowercae.\n",
        "2. Split into tokens.\n",
        "3. Remove stopwords.\n",
        "4. Stem the words into stems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bL9AoZzQLzd"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "def clean_doc(doc: str) -> List[str]:\n",
        "    # fix me!\n",
        "    return doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX0Mz1HyQLzd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfP3nogdQLzd"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgknvbexQLze"
      },
      "outputs": [],
      "source": [
        "clean_samp = [clean_doc(doc) for doc in sent_sample]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxL1nOGtQLze"
      },
      "source": [
        "## 5. Text Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl1wFMKkQLze"
      },
      "source": [
        "### Q7: Build a coprpus preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6ss9a03QLze"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQG5dClnQLze"
      },
      "outputs": [],
      "source": [
        "def dataset_to_X(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Fix me!\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN44aj90QLze"
      },
      "outputs": [],
      "source": [
        "dataset_to_X(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qSSxcnOQLze"
      },
      "source": [
        "## 6. Text Preprocessing Pipeline w/ TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RkSxs_FQLze"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmd1ZgY1QLze"
      },
      "outputs": [],
      "source": [
        "def dataset_to_tfidf_X(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Fix me!\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evnCAPrKQLzi"
      },
      "outputs": [],
      "source": [
        "tfidf_X = dataset_to_tfidf_X(dataset)\n",
        "tfidf_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoKX4gxkQLzi"
      },
      "outputs": [],
      "source": [
        "def present_active_indices(X: pd.DataFrame, index: int) -> pd.Series:\n",
        "    \"\"\"Returns a sub-series of non-zero components of the document vector at the given index.\"\"\"\n",
        "    return X.iloc[index][X.iloc[index] != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGJB0NuPQLzi"
      },
      "outputs": [],
      "source": [
        "present_active_indices(tfidf_X, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "WxZDOTS5QLzj"
      },
      "outputs": [],
      "source": [
        "present_active_indices(tfidf_X, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlO0pDiwQLzj"
      },
      "source": [
        "## 7. Use our pipeline for some simple text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf44Gm3QQLzj"
      },
      "source": [
        "We will use the `\"Liked\"` column (which is either 0 or 1) as our label, and see if we can learn to predict it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLqmAkANCp1-"
      },
      "source": [
        "### Setup our X and y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qroF7XcSCvY3"
      },
      "outputs": [],
      "source": [
        "y = dataset.iloc[:, -1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uyt2xvxQLzj"
      },
      "outputs": [],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH_VjgPzC2cd"
      },
      "source": [
        "### Split the dataset into a training and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQXYM5VzDDDI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_X, y, test_size = 0.20, random_state = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkIq23vEDIPt"
      },
      "source": [
        "### Fit a Gaussian Naive Bayes classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS9oiDXXDRdI"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JaRM7zXDWUy"
      },
      "source": [
        "### Predict on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8IGbfCZQLzk"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iif0CVhFDaMp",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "# print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoMltea5Dir1"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhlYOGlKQLzk"
      },
      "outputs": [],
      "source": [
        "# taken from https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
        "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
        "\n",
        "    Note that due to returning the created figure object, when this funciton is called in a\n",
        "    notebook the figure willl be printed twice. To prevent this, either append ; to your\n",
        "    function call, or modify the function by commenting out the return expression.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    confusion_matrix: numpy.ndarray\n",
        "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix.\n",
        "        Similarly constructed ndarrays can also be used.\n",
        "    class_names: list\n",
        "        An ordered list of class names, in the order they index the given confusion matrix.\n",
        "    figsize: tuple\n",
        "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
        "        the second determining the vertical size. Defaults to (10,7).\n",
        "    fontsize: int\n",
        "        Font size for axes labels. Defaults to 14.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    matplotlib.figure.Figure\n",
        "        The resulting confusion matrix figure\n",
        "    \"\"\"\n",
        "    df_cm = pd.DataFrame(\n",
        "        confusion_matrix, index=class_names, columns=class_names,\n",
        "    )\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    try:\n",
        "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
        "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
        "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    # Note that due to returning the created figure object, when this funciton is called in a notebook\n",
        "    # the figure willl be printed twice. To prevent this, either append ; to your function call, or\n",
        "    # modify the function by commenting out this return expression.\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj9IU6MxDnvo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r98uiEB3QLzk"
      },
      "outputs": [],
      "source": [
        "print_confusion_matrix(cm, ['Not Liked', 'Liked']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLXWv4rQQLzl"
      },
      "outputs": [],
      "source": [
        "acc = accuracy_score(y_test, y_pred)\n",
        "acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPTOY4SZQLzl"
      },
      "outputs": [],
      "source": [
        "print(f\"We achieve an accuracy score of {100*acc:.2f}%.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ca0uTxlQLzl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}